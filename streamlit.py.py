# -*- coding: utf-8 -*-
"""Streamlit.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DWYBFZ4EI6K5eVECfMlybnjoTjqxwW4O
"""



import streamlit as st
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
import json

import pickle
from collections import defaultdict,Counter

def apply_bpe_sentence(sentence, merges):
    tokens = []
    for word in sentence.strip().split():
        tokens.extend(apply_bpe_word(word, merges))
    return tokens

def load_pkl(path):
    with open(path, "rb") as f:
        return pickle.load(f)

def get_vocab(corpus):
    vocab = Counter()
    for line in corpus:
        text=line["src"]
        for word in text.strip().split():
           symbols=tuple(list(word)+["</w>"])
           vocab[symbols] += 1
    return vocab

def get_stats(vocab):
    pairs = defaultdict(int)
    for word, freq in vocab.items():
        for i in range(len(word) - 1):
            pairs[(word[i], word[i + 1])] += freq
    return pairs

def merge_vocab(pair, vocab):
    new_vocab = Counter()
    a, b = pair
    new_sym = a + b
    for word, freq in vocab.items():
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and word[i] == a and word[i + 1] == b:
                new_word.append(new_sym)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        new_vocab[tuple(new_word)] += freq
    return new_vocab

def learn_bpe(corpus, num_merges=100):
    vocab = get_vocab(corpus)
    merges = []
    for _ in range(num_merges):
        pairs = get_stats(vocab)
        if not pairs: break
        best = max(pairs, key=pairs.get)
        vocab = merge_vocab(best, vocab)
        merges.append(best)
    return merges

def apply_bpe_word(word, merges):
    symbols = list(word) + ["</w>"]
    for a, b in merges:
        i = 0
        new = []
        while i < len(symbols):
            if i < len(symbols) - 1 and symbols[i] == a and symbols[i + 1] == b:
                new.append(a + b)
                i += 2
            else:
                new.append(symbols[i])
                i += 1
        symbols = new
    return symbols

def apply_bpe(corpus, merges):
    out = []
    for line in corpus:
        tokens = []
        text=line["src"]
        for word in text.strip().split():
            tokens.extend(apply_bpe_word(word, merges))
        out.append(tokens)
    return out


# ---------------- Load vocabs ----------------
import os
import json


base_path = os.path.dirname(__file__)


src_vocab_path = os.path.join(base_path, "dataset preprocessed file", "src_vocab.json")
tgt_vocab_path = os.path.join(base_path, "dataset preprocessed file", "tgt_vocab.json")

with open(src_vocab_path, "r", encoding="utf-8") as f:
    src_token2id = json.load(f)

with open(tgt_vocab_path, "r", encoding="utf-8") as f:
    tgt_token2id = json.load(f)


src_id2token = {i: tok for tok, i in src_token2id.items()}
tgt_id2token = {i: tok for tok, i in tgt_token2id.items()}

PAD, UNK, SOS, EOS = 0, 1, 2, 3

# ---------------- Model Classes ----------------
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.5):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)
        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,
                            dropout=dropout, bidirectional=True, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.hid_dim = hid_dim
        self.n_layers = n_layers

    def forward(self, src, src_lengths):
        # src: [batch, src_len]
        embedded = self.dropout(self.embedding(src))
        # pack for efficiency
        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)
        outputs, (hidden, cell) = self.lstm(packed)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)
        # outputs: [batch, src_len, hid_dim*2]
        return outputs, hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=4, dropout=0.5):
        super().__init__()
        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)
        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,
                            dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        # input: [batch] (single timestep)
        input = input.unsqueeze(1)  # [batch, 1]
        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(1))  # [batch, output_dim]
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.teacher_forcing_ratio = teacher_forcing_ratio

        # number of layers
        self.enc_layers = encoder.n_layers
        self.dec_layers = decoder.lstm.num_layers

        # hidden sizes
        self.enc_hid_dim = encoder.hid_dim
        self.dec_hid_dim = decoder.lstm.hidden_size

        # projection (if encoder and decoder hidden dims differ)
        if self.enc_hid_dim != self.dec_hid_dim:
            self.bridge = nn.Linear(self.enc_hid_dim, self.dec_hid_dim)
        else:
            self.bridge = None

    def forward(self, src, src_lengths, trg):
        """
        src: [batch, src_len]
        src_lengths: [batch]
        trg: [batch, trg_len]
        """
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.output_dim

        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)

        # 1. Encode
        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)

        # 2. Bridge hidden states
        hidden = self._bridge_hidden(hidden)
        cell   = self._bridge_hidden(cell)

        # 3. First input to decoder = <sos>
        input = trg[:, 0]  # [batch]

        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t, :] = output
            teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[:, t] if teacher_force else top1

        return outputs

    def _bridge_hidden(self, hidden):
        """
        Convert encoder hidden -> decoder hidden shape
        Encoder hidden: [enc_layers*2, batch, enc_hid_dim]
        Decoder expects: [dec_layers, batch, dec_hid_dim]
        """
        enc_layers = hidden.size(0) // 2

        # Sum bidirectional states → [enc_layers, batch, enc_hid_dim]
        hidden = hidden[0:enc_layers] + hidden[enc_layers:2*enc_layers]

        # Project hidden dim if needed
        if self.bridge is not None:
            hidden = self.bridge(hidden)

        # Adjust number of layers
        if hidden.size(0) < self.dec_layers:
            hidden = hidden.repeat(self.dec_layers // hidden.size(0), 1, 1)
        elif hidden.size(0) > self.dec_layers:
            hidden = hidden[:self.dec_layers]

        return hidden


# ---------------- Load Model ----------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

encoder = Encoder(input_dim=len(src_token2id), emb_dim=256, hid_dim=512,
                  n_layers=2, dropout=0.3)
decoder = Decoder(output_dim=len(tgt_token2id), emb_dim=256, hid_dim=512,
                  n_layers=4, dropout=0.3)
model = Seq2Seq(encoder, decoder, device).to(device)

# Get base directory (where streamlit_app.py is located)
base_path = os.path.dirname(__file__)

# Build path to model file inside "models" folder
model_path = os.path.join(base_path, "models/Trained on emb_dim=256 adn hid_dim=512", "model_epoch30.pt")

# Load model state
model.load_state_dict(torch.load(model_path, map_location=device))

# ---------------- Translation function ----------------
def translate_sentence(model, src_ids, max_len=40):
    model.eval()
    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)
    src_lengths = torch.tensor([len(src_ids)], device=device)

    with torch.no_grad():
        enc_outputs, hidden, cell = model.encoder(src_tensor, src_lengths)
        hidden = model._bridge_hidden(hidden)
        cell   = model._bridge_hidden(cell)

    # start with <sos>
    input_tok = torch.tensor([tgt_token2id["<sos>"]], device=device)
    preds = []
    for _ in range(max_len):
        output, hidden, cell = model.decoder(input_tok, hidden, cell)
        pred = output.argmax(1).item()
        if pred == tgt_token2id["<eos>"]:
            break
        preds.append(pred)
        input_tok = torch.tensor([pred], device=device)
    return preds
def detokenize(tokens):
    return "".join(tok.replace("</w>", " ") for tok in tokens).strip()


# ---------------- Streamlit UI ----------------
st.title("Urdu → Roman Urdu Translator")
# --- Apply same BPE as training ---
with open("E:\\ANLP\\Assignment 1\\bpe_mergesur.txt", "r", encoding="utf-8") as f:
     src_merges = [tuple(line.strip().split()) for line in f if line.strip()]
user_input = st.text_area("Enter Urdu sentence:")
if st.button("Translate"):
    if not user_input.strip():
        st.warning("⚠️ Please enter a sentence.")
    else:


        src_tokens = apply_bpe_sentence(user_input.strip(), src_merges) 
        src_ids = [src_token2id.get(tok, src_token2id["<unk>"]) for tok in src_tokens]

        # --- Translate ---
        translation = translate_sentence(model, src_ids)
        pred_tokens = [tgt_id2token[i] for i in translation]

        #st.write("**Translation:**", detokenize(pred_tokens))
          
        

        # --- Show result ---
        st.markdown("**Translation:**")
        st.success(detokenize(pred_tokens))
        st.markdown("### 🔎 Debug Info")
        st.write("**Raw Input:**", user_input)
        st.write("**BPE Tokens:**", src_tokens)
        st.write("**Source IDs:**", src_ids)
        st.write("**Predicted IDs:**", translation)
        st.write("**Predicted Tokens:**", pred_tokens)

