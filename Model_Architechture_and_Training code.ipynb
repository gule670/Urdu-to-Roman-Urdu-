{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**"
      ],
      "metadata": {
        "id": "B8qrF6sgbbub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FNB11nIECIx_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,\n",
        "                            dropout=dropout, bidirectional=True, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: [batch, src_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # pack for efficiency\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        outputs, (hidden, cell) = self.lstm(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        # outputs: [batch, src_len, hid_dim*2]\n",
        "        return outputs, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wQlm3cVvrNr8"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=4, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rA10r01_rWke"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "        # number of layers\n",
        "        self.enc_layers = encoder.n_layers\n",
        "        self.dec_layers = decoder.lstm.num_layers\n",
        "\n",
        "        # hidden sizes\n",
        "        self.enc_hid_dim = encoder.hid_dim\n",
        "        self.dec_hid_dim = decoder.lstm.hidden_size\n",
        "\n",
        "\n",
        "        if self.enc_hid_dim != self.dec_hid_dim:\n",
        "            self.bridge = nn.Linear(self.enc_hid_dim, self.dec_hid_dim)\n",
        "        else:\n",
        "            self.bridge = None\n",
        "\n",
        "    def forward(self, src, src_lengths, trg):\n",
        "\n",
        "        batch_size = src.size(0)\n",
        "        trg_len = trg.size(1)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # 1. Encode\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
        "\n",
        "        # 2. Bridge hidden states\n",
        "        hidden = self._bridge_hidden(hidden)\n",
        "        cell   = self._bridge_hidden(cell)\n",
        "\n",
        "\n",
        "        input = trg[:, 0]  # [batch]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t, :] = output\n",
        "            teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _bridge_hidden(self, hidden):\n",
        "\n",
        "        enc_layers = hidden.size(0) // 2\n",
        "\n",
        "\n",
        "        hidden = hidden[0:enc_layers] + hidden[enc_layers:2*enc_layers]\n",
        "\n",
        "\n",
        "        if self.bridge is not None:\n",
        "            hidden = self.bridge(hidden)\n",
        "\n",
        "\n",
        "        if hidden.size(0) < self.dec_layers:\n",
        "            hidden = hidden.repeat(self.dec_layers // hidden.size(0), 1, 1)\n",
        "        elif hidden.size(0) > self.dec_layers:\n",
        "            hidden = hidden[:self.dec_layers]\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fi9mr-PrtUG",
        "outputId": "0c112dc4-91c3-4b0b-967e-d2e788e895bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/Preprocessed.zip\n",
            "  inflating: Preprocessed/dataset_full.pkl  \n",
            " extracting: Preprocessed/entoken.zip  \n",
            "   creating: Preprocessed/entoken/\n",
            "  inflating: Preprocessed/entoken/bpe_merges.txt  \n",
            "  inflating: Preprocessed/entoken/tgt_bpe.json  \n",
            "  inflating: Preprocessed/test.pkl   \n",
            "  inflating: Preprocessed/train.pkl  \n",
            " extracting: Preprocessed/urtoken.zip  \n",
            "   creating: Preprocessed/urtoken/\n",
            "  inflating: Preprocessed/urtoken/bpe_mergesur.txt  \n",
            "  inflating: Preprocessed/urtoken/tgt_bpeur.json  \n",
            "  inflating: Preprocessed/valid.pkl  \n"
          ]
        }
      ],
      "source": [
        "!unzip  /content/Preprocessed.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4bH_KpAsJbP"
      },
      "source": [
        "**Training and Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw08MGpWsToc",
        "outputId": "6c632f94-db2d-4869-8c0b-ce123d9e818e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 21003 parallel sentences.\n",
            "Source vocab size: 4905\n",
            "Target vocab size: 4916\n",
            "Example encoded pair: ([4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 3])\n",
            "Saved splits and vocab files:\n",
            "Train: 10501 Valid: 5251 Test: 5251\n",
            "src_vocab.json / tgt_vocab.json saved with sizes: 4905 4916\n"
          ]
        }
      ],
      "source": [
        "import json, pickle, random\n",
        "from collections import Counter\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "SRC_FILE = \"/content/tgt_bpeur.json\"   # Urdu tokenized\n",
        "TGT_FILE = \"/content/tgt_bpe.json\"     # Roman Urdu tokenized\n",
        "\n",
        "OUT_DIR = \"/content\"   # where train.pkl / valid.pkl / test.pkl will be saved\n",
        "\n",
        "# ---------------- Load tokenized sentences ----------------\n",
        "with open(SRC_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    src_sentences = json.load(f)   # list of lists\n",
        "\n",
        "with open(TGT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    tgt_sentences = json.load(f)\n",
        "\n",
        "assert len(src_sentences) == len(tgt_sentences), \"Mismatch between src and tgt!\"\n",
        "print(f\"Loaded {len(src_sentences)} parallel sentences.\")\n",
        "\n",
        "# ---------------- Build vocabulary from sentences ----------------\n",
        "def build_vocab(sentences, min_freq=1):\n",
        "    counter = Counter(tok for sent in sentences for tok in sent)\n",
        "    # special tokens always at the start\n",
        "    tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "    tokens += [tok for tok, c in counter.items() if c >= min_freq]\n",
        "    token2id = {tok: idx for idx, tok in enumerate(tokens)}\n",
        "    id2token = {idx: tok for tok, idx in token2id.items()}\n",
        "    return token2id, id2token\n",
        "\n",
        "src_token2id, src_id2token = build_vocab(src_sentences)\n",
        "tgt_token2id, tgt_id2token = build_vocab(tgt_sentences)\n",
        "\n",
        "print(f\"Source vocab size: {len(src_token2id)}\")\n",
        "print(f\"Target vocab size: {len(tgt_token2id)}\")\n",
        "\n",
        "# ---------------- Encode sentences ----------------\n",
        "def encode(tokens, vocab):\n",
        "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n",
        "\n",
        "pairs = []\n",
        "for src, tgt in zip(src_sentences, tgt_sentences):\n",
        "    src_ids = encode(src, src_token2id)\n",
        "    tgt_ids = [tgt_token2id[\"<sos>\"]] + encode(tgt, tgt_token2id) + [tgt_token2id[\"<eos>\"]]\n",
        "    pairs.append((src_ids, tgt_ids))\n",
        "\n",
        "print(\"Example encoded pair:\", pairs[0])\n",
        "\n",
        "# ---------------- Split into train/valid/test ----------------\n",
        "random.shuffle(pairs)\n",
        "n = len(pairs)\n",
        "train = pairs[:int(0.5*n)]\n",
        "valid = pairs[int(0.5*n):int(0.75*n)]\n",
        "test  = pairs[int(0.75*n):]\n",
        "\n",
        "with open(f\"{OUT_DIR}/train.pkl\", \"wb\") as f: pickle.dump(train, f)\n",
        "with open(f\"{OUT_DIR}/valid.pkl\", \"wb\") as f: pickle.dump(valid, f)\n",
        "with open(f\"{OUT_DIR}/test.pkl\",  \"wb\") as f: pickle.dump(test, f)\n",
        "\n",
        "# ---------------- Save vocabs ----------------\n",
        "with open(f\"{OUT_DIR}/src_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(src_token2id, f, ensure_ascii=False, indent=2)\n",
        "with open(f\"{OUT_DIR}/tgt_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tgt_token2id, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved splits and vocab files:\")\n",
        "print(\"Train:\", len(train), \"Valid:\", len(valid), \"Test:\", len(test))\n",
        "print(\"src_vocab.json / tgt_vocab.json saved with sizes:\",\n",
        "      len(src_token2id), len(tgt_token2id))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiozviNwhvbv",
        "outputId": "975975ed-c1d9-48e0-e79b-68d6d4382a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([56, 649, 56, 1953, 68, 42, 2237, 363, 3366], [2, 2930, 2931, 1881, 2327, 2143, 2144, 45, 2445, 395, 3548, 3])\n",
            "([259, 67, 12, 109, 2748, 62, 281, 76], [2, 274, 74, 12, 116, 696, 1299, 69, 298, 83, 3])\n",
            "([116, 2001, 71, 230, 74, 356, 622, 10, 295], [2, 123, 2200, 78, 333, 81, 387, 682, 10, 313, 3])\n",
            "([1451, 314, 1621, 712, 1541, 3097, 3097, 109], [2, 1596, 337, 1781, 780, 1271, 149, 341, 2043, 999, 341, 3982, 116, 3])\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"/content/train.pkl\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "for p in pairs[:4]:\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yUd8IOU1NmN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# ---------------- Dataset Wrapper ----------------\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        src, trg = self.pairs[idx]\n",
        "        return torch.tensor(src, dtype=torch.long), torch.tensor(trg, dtype=torch.long)\n",
        "\n",
        "# ---------------- Loss & Optimizer ----------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_model(model, train_data, valid_data, epochs=10, lr=1e-3, batch_size=64):\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
        "    valid_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=lambda x: x)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # assume PAD=0\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- Train ----\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            src, trg = zip(*batch)\n",
        "            src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0).to(device)\n",
        "            trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=0).to(device)\n",
        "            src_lengths = torch.tensor([len(s) for s in src]).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, src_lengths, trg)\n",
        "\n",
        "            # Shift target for teacher forcing\n",
        "            output_dim = output.shape[-1]\n",
        "            loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                src, trg = zip(*batch)\n",
        "                src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0).to(device)\n",
        "                trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=0).to(device)\n",
        "                src_lengths = torch.tensor([len(s) for s in src]).to(device)\n",
        "\n",
        "                output = model(src, src_lengths, trg)\n",
        "                output_dim = output.shape[-1]\n",
        "                loss = criterion(output[:, 1:].reshape(-1, output_dim), trg[:, 1:].reshape(-1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={total_loss/len(train_loader):.3f}, \"\n",
        "              f\"Val Loss={val_loss/len(valid_loader):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1chbey-gyQy",
        "outputId": "90e3c614-9605-4610-e7a4-b6c013be1380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/PreprocessedR.zip\n",
            "  inflating: Preprocessed/dataset_full.pkl  \n",
            "   creating: Preprocessed/entoken/\n",
            "  inflating: Preprocessed/entoken/bpe_merges.txt  \n",
            "  inflating: Preprocessed/entoken/tgt_bpe.json  \n",
            "  inflating: Preprocessed/src_vocab.json  \n",
            "  inflating: Preprocessed/test.pkl   \n",
            "  inflating: Preprocessed/tgt_vocab.json  \n",
            "  inflating: Preprocessed/train.pkl  \n",
            "   creating: Preprocessed/urtoken/\n",
            "  inflating: Preprocessed/urtoken/bpe_mergesur.txt  \n",
            "  inflating: Preprocessed/urtoken/tgt_bpeur.json  \n",
            "  inflating: Preprocessed/valid.pkl  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/PreprocessedR.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PvRBMUkQ4YdV"
      },
      "outputs": [],
      "source": [
        "import pickle, random\n",
        "INPUT_DIM = len(src_token2id)\n",
        "OUTPUT_DIM = len(tgt_token2id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model with emb_dim=256, hid_dim=512 and droput=0.5, encoder layers=2 decoder layers = 4"
      ],
      "metadata": {
        "id": "McmMSHy0cc-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import pickle, os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs): self.pairs = pairs\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, i): return self.pairs[i]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src,tgt = zip(*batch)\n",
        "    src = [torch.tensor(s) for s in src]\n",
        "    tgt = [torch.tensor(t) for t in tgt]\n",
        "    return (nn.utils.rnn.pad_sequence(src,batch_first=True,padding_value=0),\n",
        "            nn.utils.rnn.pad_sequence(tgt,batch_first=True,padding_value=0))\n",
        "\n",
        "# ---------------- Load Data ----------------\n",
        "train = pickle.load(open(\"/content/train.pkl\",\"rb\"))\n",
        "valid = pickle.load(open(\"/content/valid.pkl\",\"rb\"))\n",
        "test  = pickle.load(open(\"/content/test.pkl\",\"rb\"))\n",
        "\n",
        "train_loader = DataLoader(TranslationDataset(train), batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(TranslationDataset(valid), batch_size=64, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "4DKU0Z75ohhg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------- Model ----------------\n",
        "\n",
        "INPUT_DIM, OUTPUT_DIM = len(src_token2id), len(tgt_token2id)\n",
        "encoder = Encoder(INPUT_DIM, 256, 512, n_layers=2, dropout=0.5)\n",
        "decoder = Decoder(OUTPUT_DIM, 256, 512, n_layers=4, dropout=0.5)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "metadata": {
        "id": "ZuH6gwGAouM3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Translation ----------------\n",
        "def translate_sentence(model, src_ids, max_len=40):\n",
        "    model.eval()\n",
        "    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
        "    src_lengths = torch.tensor([len(src_ids)], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_outputs, hidden, cell = model.encoder(src_tensor, src_lengths)\n",
        "        hidden = model._bridge_hidden(hidden)\n",
        "        cell   = model._bridge_hidden(cell)\n",
        "\n",
        "    # start with <sos>\n",
        "    input_tok = torch.tensor([tgt_token2id[\"<sos>\"]], device=device)\n",
        "    preds = []\n",
        "    for _ in range(max_len):\n",
        "        output, hidden, cell = model.decoder(input_tok, hidden, cell)\n",
        "        pred = output.argmax(1).item()\n",
        "        if pred == tgt_token2id[\"<eos>\"]:\n",
        "            break\n",
        "        preds.append(pred)\n",
        "        input_tok = torch.tensor([pred], device=device)\n",
        "    return preds\n",
        "def detokenize(tokens):\n",
        "    return \"\".join(tok.replace(\"</w>\", \" \") for tok in tokens).strip()"
      ],
      "metadata": {
        "id": "2dhclB9Dpgbj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ---------------- Training ----------------\n",
        "# ---------------- Training epoch function ----------------\n",
        "def train_epoch(loader):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        # lengths for packed sequences\n",
        "        src_lengths = torch.tensor([len(s[s!=0]) for s in src], device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src_lengths, tgt)   # forward pass\n",
        "\n",
        "        # shift for loss (ignore first token <sos>)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(\n",
        "            output[:, 1:].reshape(-1, output_dim),\n",
        "            tgt[:, 1:].reshape(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "\n",
        "    return total / len(loader)\n",
        "\n",
        "# ---------------- Training loop ----------------\n",
        "N_EPOCHS = 10\n",
        "for epoch in range(N_EPOCHS):\n",
        "    tr_loss = train_epoch(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS} - Train Loss: {tr_loss:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "src_ids = [src_token2id.get(tok, src_token2id[\"<unk>\"]) for tok in [\"آپ\",\"کیسے\",\"ہیں\",\"؟\"]]\n",
        "pred_ids = translate_sentence(model, src_ids)\n",
        "pred_tokens = [tgt_id2token[i] for i in pred_ids]\n",
        "print(\"Prediction:\", detokenize(pred_tokens))\n",
        "\n",
        "# ---------------- BLEU Evaluation ----------------\n",
        "def evaluate_bleu(model, loader, n_batches=5):\n",
        "    smooth = SmoothingFunction().method1\n",
        "    scores=[]\n",
        "    for i,(src,tgt) in enumerate(loader):\n",
        "        if i>=n_batches: break\n",
        "        for s,t in zip(src,tgt):\n",
        "            s = s[s!=0].tolist()\n",
        "            t = t[t!=0].tolist()\n",
        "            pred_ids = translate_sentence(model, s)\n",
        "            ref = [tgt_id2token[i] for i in t[1:-1]]   # drop <sos>, <eos>\n",
        "            hyp = [tgt_id2token[i] for i in pred_ids]\n",
        "            if len(hyp)>0:\n",
        "                scores.append(sentence_bleu([ref], hyp, smoothing_function=smooth))\n",
        "    return sum(scores)/len(scores) if scores else 0.0"
      ],
      "metadata": {
        "id": "8GUHA4WCn7wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------------- Run Training ----------------\n",
        "N_EPOCHS = 30\n",
        "os.makedirs(\"/content/checkpoints\", exist_ok=True)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    tr_loss = train_epoch(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{N_EPOCHS} - train_loss={tr_loss:.3f}\")\n",
        "\n",
        "    if (epoch+1)%5==0:\n",
        "        bleu = evaluate_bleu(model, valid_loader)\n",
        "        print(f\"  >> Validation BLEU: {bleu:.4f}\")\n",
        "        torch.save(model.state_dict(), f\"/content/checkpoints/model_epoch{epoch+1}.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovcgu_NFsoFd",
        "outputId": "e06ae5d5-919a-4a00-c46d-e82dd9573d5f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 6.530\n",
            "Epoch 2/10 - Train Loss: 5.736\n",
            "Epoch 3/10 - Train Loss: 5.293\n",
            "Epoch 4/10 - Train Loss: 4.939\n",
            "Epoch 5/10 - Train Loss: 4.611\n",
            "Epoch 6/10 - Train Loss: 4.324\n",
            "Epoch 7/10 - Train Loss: 4.052\n",
            "Epoch 8/10 - Train Loss: 3.810\n",
            "Epoch 9/10 - Train Loss: 3.580\n",
            "Epoch 10/10 - Train Loss: 3.359\n",
            "Prediction: lab lab lab\n",
            "Epoch 1/30 - train_loss=3.157\n",
            "Epoch 2/30 - train_loss=2.966\n",
            "Epoch 3/30 - train_loss=2.804\n",
            "Epoch 4/30 - train_loss=2.643\n",
            "Epoch 5/30 - train_loss=2.496\n",
            "  >> Validation BLEU: 0.0983\n",
            "Epoch 6/30 - train_loss=2.354\n",
            "Epoch 7/30 - train_loss=2.227\n",
            "Epoch 8/30 - train_loss=2.103\n",
            "Epoch 9/30 - train_loss=1.983\n",
            "Epoch 10/30 - train_loss=1.894\n",
            "  >> Validation BLEU: 0.1232\n",
            "Epoch 11/30 - train_loss=1.786\n",
            "Epoch 12/30 - train_loss=1.699\n",
            "Epoch 13/30 - train_loss=1.605\n",
            "Epoch 14/30 - train_loss=1.532\n",
            "Epoch 15/30 - train_loss=1.448\n",
            "  >> Validation BLEU: 0.1384\n",
            "Epoch 16/30 - train_loss=1.375\n",
            "Epoch 17/30 - train_loss=1.313\n",
            "Epoch 18/30 - train_loss=1.245\n",
            "Epoch 19/30 - train_loss=1.185\n",
            "Epoch 20/30 - train_loss=1.127\n",
            "  >> Validation BLEU: 0.1468\n",
            "Epoch 21/30 - train_loss=1.082\n",
            "Epoch 22/30 - train_loss=1.024\n",
            "Epoch 23/30 - train_loss=0.981\n",
            "Epoch 24/30 - train_loss=0.928\n",
            "Epoch 25/30 - train_loss=0.883\n",
            "  >> Validation BLEU: 0.1529\n",
            "Epoch 26/30 - train_loss=0.852\n",
            "Epoch 27/30 - train_loss=0.810\n",
            "Epoch 28/30 - train_loss=0.775\n",
            "Epoch 29/30 - train_loss=0.737\n",
            "Epoch 30/30 - train_loss=0.715\n",
            "  >> Validation BLEU: 0.1490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdcjWQIRPrS7"
      },
      "source": [
        "Evaluation and Testing of model trained on emb_dim=256, hid_dim=512"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_tokens = 0, 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        src_lengths = torch.tensor([len(s[s!=0]) for s in src], device=device)\n",
        "\n",
        "        output = model(src, src_lengths, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        loss = criterion(\n",
        "            output[:, 1:].reshape(-1, output_dim),\n",
        "            tgt[:, 1:].reshape(-1)\n",
        "        )\n",
        "        # accumulate loss * number of tokens\n",
        "        total_loss += loss.item() * (tgt[:,1:].numel())\n",
        "        total_tokens += tgt[:,1:].numel()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return math.exp(avg_loss)   # perplexity\n"
      ],
      "metadata": {
        "id": "kbm3RLTDnWJx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(INPUT_DIM, 256, 512, n_layers=2, dropout=0.3)\n",
        "decoder = Decoder(OUTPUT_DIM, 256, 512, n_layers=4, dropout=0.3)\n",
        "model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=0.5).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/model_epoch30.pt\", map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A14c_ixrnljm",
        "outputId": "20502016-bda9-410e-af60-794ce65986d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ppl = evaluate_loss(model, valid_loader)\n",
        "print(f\"Validation Perplexity: {ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxTUwjBznYci",
        "outputId": "6318106d-b88d-4a97-b355-37b4c3f59dab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Perplexity: 10.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install editdistance\n",
        "import editdistance\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_cer(model, loader, n_batches=5):\n",
        "    model.eval()\n",
        "    total_dist, total_chars = 0, 0\n",
        "    for i, (src, tgt) in enumerate(loader):\n",
        "        if i >= n_batches: break\n",
        "        for s, t in zip(src, tgt):\n",
        "            s = s[s!=0].tolist()\n",
        "            t = t[t!=0].tolist()\n",
        "            pred_ids = translate_sentence(model, s)\n",
        "\n",
        "            # detokenize to strings\n",
        "            ref = detokenize([tgt_id2token[i] for i in t[1:-1]])\n",
        "            hyp = detokenize([tgt_id2token[i] for i in pred_ids])\n",
        "\n",
        "            # edit distance\n",
        "            dist = editdistance.eval(hyp, ref)\n",
        "            total_dist += dist\n",
        "            total_chars += len(ref)\n",
        "\n",
        "    return total_dist / total_chars if total_chars > 0 else 0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrO-80oQo3qt",
        "outputId": "39a92e2b-071b-4e1a-85d8-713ff91647e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cer = evaluate_cer(model, valid_loader, n_batches=5)\n",
        "print(f\"Validation CER: {cer:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wwbnLJupPyo",
        "outputId": "672a8b35-1cef-46cb-e952-7f61fbd66f6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation CER: 0.2789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cer = evaluate_cer(model, valid_loader, n_batches=5)\n",
        "print(f\"Validation CER: {cer:.4f}\")"
      ],
      "metadata": {
        "id": "08iSW6Lro_tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In Case of further training**"
      ],
      "metadata": {
        "id": "0YTWuohtqoX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(INPUT_DIM, 256, 512, n_layers=2, dropout=0.3)\n",
        "decoder = Decoder(OUTPUT_DIM, 256, 512, n_layers=4, dropout=0.3)\n",
        "model = Seq2Seq(encoder, decoder, device, teacher_forcing_ratio=0.5).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/checkpoints/model_epoch30.pt\", map_location=device))\n",
        "\n",
        "# Recreate optimizer with lower LR\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4,weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_token2id[\"<pad>\"])\n",
        "\n",
        "start_epoch = 30   # where you left off\n",
        "N_EPOCHS = 20      # train 20 more epochs"
      ],
      "metadata": {
        "id": "D9gIBQ5DsZ-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval(); total=0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        src_lengths = torch.tensor([len(s[s!=0]) for s in src], device=device)\n",
        "        output = model(src, src_lengths, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(\n",
        "            output[:,1:].reshape(-1, output_dim),\n",
        "            tgt[:,1:].reshape(-1)\n",
        "        )\n",
        "        total += loss.item()\n",
        "    return total/len(loader)\n",
        "for epoch in range(start_epoch+1, start_epoch+N_EPOCHS+1):\n",
        "    tr_loss = train_epoch(train_loader)\n",
        "    val_loss = evaluate(valid_loader)\n",
        "\n",
        "    # Decay teacher forcing ratio\n",
        "    model.teacher_forcing_ratio = max(0.1, model.teacher_forcing_ratio * 0.9)\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss: {tr_loss:.3f} | Val Loss: {val_loss:.3f} | TF={model.teacher_forcing_ratio:.2f}\")\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        bleu = evaluate_bleu(valid_loader, n_samples=200)\n",
        "        print(f\"   Validation BLEU: {bleu*100:.2f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save(model.state_dict(), f\"model_epoch{epoch}.pt\")"
      ],
      "metadata": {
        "id": "ONJlzcOJqre2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Qt7e9AGvEi"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}